## Relatório de Teste Prático de ML

Comecei fazendo uma análise exploratória dos dados. Vi a quantidade de arquivos em cada diretório do dataset. Pelo file explorer do meu sistema operacional, visualizei várias imagens para ter uma ideia do conteúdo. Percebi que havia algumas imagens bem escuras, que sob análise detalhada percebi serem completamente pretas. Criei um script para remover essas imagens da pasta train. Percebi também que havia imagens duplicadas, não só dentro de um diretório, como entre diretórios. Algumas das duplicatas eram porque havia múltiplos cachorros na imagem, de raças diferentes; essas imagens mantive. Já as imagens duplicadas dentro de um mesmo diretório, removi. Criei um arquivo clean_dataset.py para juntar essa funcionalidade e a de remover imagens pretas.

Criei um script para fazer download do dataset e extraí-lo: download_dataset.py, para facilitar a reprodução dos treinos e testes.

Em seguida, examinei a quantidade de amostras por classe no train. A classe com menos amostras tem 145 amostras, e a com mais amostras, 252. Julguei que estão relativamente balanceadas, e portanto não usei nenhuma técnica para tratar class imbalance.

Em seguida, escolhi o framework fastai (biblioteca que usa o PyTorch), devido a algumas facilidades que ela proporciona: bons defaults para fine-tuning de redes, como: uso de diferentes learning rates para cada camada da rede que se está fazendo fine-tuning (camadas mais próximas da entrada devem ter learning rates menores), e facilidade de encontrar um learning rate perto do ótimo (método lr_find do objeto a ser treinado).

Para a Parte 1, treinei alguns classificadores para as 100 raças, usando redes pré-treinadas no ImageNet. As imagens do ImageNet tem características parecidas com as do dataset de cachorros; por exemplo, o objeto de interesse geralmente está no centro da imagem. Julguei então que uma rede pré-treinada no ImageNet seria um bom ponto de partida. Treinei algumas redes diferentes, como Resnet50, alguns tamanhos de DenseNet, MobileNetv2, e por fim a rede 'resnext50_32x4d'. Essa foi a que teve o melhor desempenho, com cerca de 3% de acurácia no validation set. Julguei ser um bom resultado, considerando que há 100 classes, e muitas raças de cachorros são parecidas entre si.

Na Parte 2, identifiquei imediatamente que seria interessante usar a rede treinada na Parte 1 para calcular um embedding (vetor descritor) de cada imagem, e usar esses embeddings para fazer o enroll de novas imagens e classificação das imagens de teste. Removi as últimas camadas da rede resnext50, logo depois de uma operação AdaptiveAvgPool2d, obtendo assim um descritor com 2048 dimensões para cada imagem.

Minha primeira ideia para fazer a classificação dos descritores foi usar um SVM linear. É um classificador comumente empregado para esse tipo de problema. Porém, seria necessário retreiná-lo para cada nova imagem cadastrada na base de busca. O treinamento de um SVM linear é um processo rápido, e possivelmente ficaria abaixo de 1 
segundo, mas julguei que não estaria seguindo o espírito do Teste Prático, que pede uma solução que possa ser retreinada de forma incremental.

Mesmo assim, experimentei treinar um SVM linear com a biblioteca scikit-learn, com todas as imagens da pasta enroll, e calculei a acurácia para a pasta teste, obtendo acima de 98%. Para a Parte 3, tratei o problema de imagens de raças desconhecidas aplicando um limiar na saída de probabilidade do SVM linear (para tanto, foi necessário treinar o SVM para que emitisse probabilidades na saída, o que não é uma funcionalidade "nativa" do SVM, e sim um processo de calibração de probabilidade feito via cross-validation).

De forma a ter uma solução que possa ser treinada de forma incremental, escolhi modelar como um problema de k-nearest neighbors. É um algoritmo fácil de implementar do zero, mas visando uma solução que pudesse ser usada em escala, escolhi a biblioteca `faiss` do Facebook, uma biblioteca para busca de similaridade eficiente e clustering de vetores densos. O `faiss` permite adicionar vetores de forma incremental. Além de busca exata de nearest neighbors, implementa também algoritmos aproximados, que podem ser mais apropriados para uso em escala muito grande. Escolhi usar distância cosseno ao invés de distância euclidiana, pois em um projeto anterior tive bons resultados com ela.

A minha maior dúvida em todo o Teste Prático foi como detectar anomalias (amostras de classe desconhecida) no modelo de nearest neighbors. Acabei utilizando duas formas em conjunto. A primeira consiste em, no processo de busca, indicar que não foi possível encontrar matches, caso não haja consenso de maioria nas k amostras mais próximas. A segunda forma foi estabelecer um limiar de distância para que cada vizinho seja considerado próximo o suficiente; o limiar foi determinado empiricamente. Uma desvantagem de usar detecção de anomalia é que inevitavelmente aumenta-se a chance de rejeitar classificações corretas de amostras com classe conhecida. De fato, observei que a acurácia de testes caiu ao implementar as duas formas de rejeição.


